ogbl-citation2:
  # M_nodes: 64
  # adaptive: false
  add_source: false
  adjoint: true
  adjoint_method: rk4
  adjoint_step_size: 1
  alpha: 1.0
  # alpha_dim: sc
  att_samp_pct: 0.8105268910037231
  attention_dim: 32
  attention_norm_idx: 1
  # attention_rewiring: false
  attention_type: scaled_dot
  augment: false
  # baseline: false
  batch_norm: true
  beltrami: true
  # beta_dim: sc
  block: hard_attention
  # cpus: 1
  data_norm: rw
  dataset: ogbl-citation2
  decay: 0.001 ###################
  directional_penalty: null
  dropout: 0.3 #######################
  # dt: 0.001
  # dt_min: 1e-05
  epoch: 200
  exact: false
  fc_out: false
  feat_hidden_dim: 64
  function: laplacian #transformer            # GAT demonstrate very bad perfomance
  # gdc_avg_degree: 64
  gdc_k: 64
  gdc_method: ppr #####################
  gdc_sparsification: topk ########################
  gdc_threshold: 0.01
  # gpus: 1.0
  # grace_period: 20
  heads: 2 ###########################
  heat_time: 3.0
  hidden_dim: 128
  input_dropout: 0
  jacobian_norm2: null
  kinetic_energy: true
  # label_rate: 0.21964773835397075
  leaky_relu_slope: 0.2
  lr: 0.001 #########################
  # max_epochs: 1000
  max_iters: 100
  max_nfe: 500
  method: rk4
  # metric: MRR
  mix_features: false
  # name: collab_beltrami_hard_att
  new_edges: random ############################
  no_alpha_sigmoid: false
  # not_lcc: false
  # num_init: 2
  # num_samples: 200
  # num_splits: 1
  # ode_blocks: 1
  optimizer: rmsprop
  # patience: 100
  pos_enc_hidden_dim: 64
  pos_enc_orientation: row ###################
  pos_enc_type: GDC
  ppr_alpha: 0.05
  # reduction_factor: 10
  # regularise: false
  reweight_attention: false
  rewire_KNN: false
  rewire_KNN_T: T0 ########################
  rewire_KNN_epoch: 5
  rewire_KNN_k: 16 #64
  rewire_KNN_sym: true # false
  rewiring: null
  rw_addD: 0.02
  # rw_rmvR: 0.02
  self_loop_weight: 1
  sparsify: S_hat
  square_plus: false
  step_size: 1
  threshold_type: addD_rvR #################
  time: 3 #3.6760155951687636 ####################### TRY TO CHANGE IT
  tol_scale: 11353.558848254957
  tol_scale_adjoint: 1.0
  total_deriv: null
  # use_cora_defaults: false
  use_flux: false
  # use_labels: false
  # use_lcc: true
  use_mlp: false

  # Additional hyperparameters
  batch_size: 8192
  dataset_dir: ./dataset
  fa_layer: false
  gcn: false
  mlp_num_layers: 3
  no_early: true
  use_valedges_as_input: false
  pos_enc_csv: False